SHELL := /bin/bash

.PHONY: help bootstrap install-dev lint test format scrape-sample dev dev-local dev-clean stop api api-dev api-sqlite api-seed api-seed-full backend scraper scraper-details export-active-fighters export-active-fighters-sample scrape-sherdog-search verify-sherdog-matches verify-sherdog-matches-auto scrape-sherdog-images update-fighter-images sherdog-workflow sherdog-workflow-auto sherdog-workflow-sample scrape-images-wikimedia scrape-images-wikimedia-test scrape-images-orchestrator scrape-images-orchestrator-test scrape-images-orchestrator-all sync-images-to-db review-recent-images remove-bad-images frontend db-upgrade db-downgrade db-reset load-data load-data-sample load-data-details load-data-dry-run load-data-details-dry-run reload-data update-records champions-scrape champions-refresh scraper-events scraper-events-details scraper-events-details-sample load-events load-events-sample load-events-dry-run load-events-details load-events-details-sample load-events-details-dry-run tunnel-frontend tunnel-api tunnel-stop deploy deploy-config deploy-build deploy-test deploy-check ensure-docker docker-up docker-down docker-status

help:
	@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | sort | awk 'BEGIN {FS = ":.*?## "}; {printf "\033[36m%-20s\033[0m %s\n", $$1, $$2}'

ensure-docker: ## Ensure Docker services (PostgreSQL, Redis) are running
	@if ! docker compose ps | grep -q "redis.*Up"; then \
		echo "ğŸ³ Starting Docker services (PostgreSQL + Redis)..."; \
		docker compose up -d; \
		echo "â³ Waiting for services to be ready..."; \
		sleep 3; \
		echo "âœ… Docker services started"; \
	else \
		echo "âœ… Docker services already running"; \
	fi

docker-up: ## Start Docker services (PostgreSQL + Redis)
	docker compose up -d
	@echo "âœ… Docker services started"

docker-down: ## Stop Docker services
	docker compose down
	@echo "âœ… Docker services stopped"

docker-status: ## Show status of Docker services
	docker compose ps

bootstrap: ## Install Python and Node dependencies
	uv sync --all-extras
	cd frontend && pnpm install || npm install

install-dev: ## Install development-only dependencies
	uv sync --extra dev

lint: ## Run linters for Python and frontend
	ruff check scraper backend
	cd frontend && pnpm lint || npm run lint

format: ## Format Python and frontend code
	ruff format scraper backend
	cd frontend && pnpm format || npm run format

test: ## Run unit tests across the repo
	pytest
	cd frontend && pnpm test || npm test

scrape-sample: ## Run sample scrape to populate data/samples
	python -m scripts.scrape_sample

dev: ensure-docker ## Start backend, frontend, and Cloudflare tunnels with auto-config
	@echo ""
	@echo "ğŸ”„ Stopping existing processes..."
	@pkill cloudflared 2>/dev/null || true
	@lsof -ti :3000 | xargs kill -9 2>/dev/null || true
	@lsof -ti :8000 | xargs kill -9 2>/dev/null || true
	@sleep 1
	@echo ""
	@echo "ğŸš‡ Starting Cloudflare tunnels..."
	@TUNNEL_OUTPUT=$$(bash scripts/start_tunnels.sh); \
	FRONTEND_URL=$$(echo "$$TUNNEL_OUTPUT" | grep "FRONTEND_URL=" | cut -d'=' -f2); \
	API_URL=$$(echo "$$TUNNEL_OUTPUT" | grep "API_URL=" | cut -d'=' -f2); \
	echo ""; \
	echo "ğŸ“ Updating configuration files..."; \
	sed -i.bak "s|CORS_ALLOW_ORIGINS=.*|CORS_ALLOW_ORIGINS=$$FRONTEND_URL|" .env && rm .env.bak; \
	sed -i.bak "s|NEXT_PUBLIC_API_BASE_URL=.*|NEXT_PUBLIC_API_BASE_URL=$$API_URL|" frontend/.env.local && rm frontend/.env.local.bak; \
	echo ""; \
	echo "ğŸš€ Starting services..."; \
	trap 'pkill cloudflared 2>/dev/null || true; kill 0' INT TERM EXIT; \
	.venv/bin/uvicorn backend.main:app --reload --host $${API_HOST:-0.0.0.0} --port $${API_PORT:-8000} > /tmp/backend.log 2>&1 & \
	sleep 2; \
	echo "ğŸ”§ Generating TypeScript types from OpenAPI (background)..."; \
	mkdir -p frontend/src/lib/generated; \
	( \
	  for i in 1 2 3 4 5 6 7 8 9 10; do \
	    if curl -sf http://localhost:8000/health > /dev/null 2>&1; then \
	      echo "âœ… Backend ready for type generation"; \
	      cd frontend && pnpm generate:types > /tmp/types.log 2>&1 \
	        && echo "âœ… Types generated" \
	        || echo "âš ï¸  Type generation failed (see /tmp/types.log)"; \
	      exit 0; \
	    fi; \
	    sleep 0.5; \
	  done; \
	  echo "âš ï¸  Backend not ready after 5s, skipping type generation"; \
	) & \
	DEV_CMD=$${NEXT_DEV_CMD:-dev}; \
	cd frontend && pnpm run "$$DEV_CMD" > /tmp/frontend.log 2>&1 & \
	sleep 3; \
	echo ""; \
	echo "âœ… All services started!"; \
	echo ""; \
	echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"; \
	echo "ğŸ“ LOCAL URLs:"; \
	echo "   Frontend: http://localhost:3000"; \
	echo "   Backend:  http://localhost:8000"; \
	echo ""; \
	echo "ğŸŒ PUBLIC URLs (Cloudflare Tunnels):"; \
	echo "   Frontend: $$FRONTEND_URL"; \
	echo "   Backend:  $$API_URL"; \
	echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"; \
	echo ""; \
	echo "Press Ctrl+C to stop all services"; \
	tail -f /tmp/backend.log /tmp/frontend.log

api: ensure-docker ## Start only the FastAPI backend (kills existing process on port 8000)
	@echo ""
	@echo "Stopping any existing process on port 8000..."
	@lsof -ti :8000 | xargs kill -9 2>/dev/null || true
	@sleep 1
	@echo "Starting backend..."
	@.venv/bin/uvicorn backend.main:app --reload --host $${API_HOST:-0.0.0.0} --port $${API_PORT:-8000}

api-dev: ## Start backend with auto-reload (without Docker - uses SQLite if DATABASE_URL unset)
	@echo ""
	@echo "ğŸš€ Starting backend in development mode (SQLite if DATABASE_URL not set)..."
	@lsof -ti :8000 | xargs kill -9 2>/dev/null || true
	@sleep 1
	@uv run uvicorn backend.main:app --reload --host 0.0.0.0 --port 8000

api-sqlite: ## Start backend with SQLite (forced, no Docker required)
	@echo ""
	@echo "ğŸ—„ï¸  Starting backend with SQLite (forced mode)..."
	@lsof -ti :8000 | xargs kill -9 2>/dev/null || true
	@sleep 1
	@USE_SQLITE=1 uv run uvicorn backend.main:app --reload --host 0.0.0.0 --port 8000

api-seed: ## Seed database with sample fighters from fixtures
	@echo "ğŸŒ± Seeding database with sample fighters..."
	@uv run python -m backend.scripts.seed_fighters ./data/fixtures/fighters.jsonl

api-seed-full: ## Seed database with all fighters from scraped data
	@echo "ğŸŒ± Seeding database with all fighters (this may take a while)..."
	@uv run python -m backend.scripts.seed_fighters ./data/processed/fighters_list.jsonl

scraper: ## Run full scraper crawl (fighters list)
	.venv/bin/scrapy crawl fighters_list

scraper-details: ## Run scraper for fighter details (all fighters)
	.venv/bin/scrapy crawl fighter_detail -a input_file="data/processed/fighters_list.jsonl"

scraper-details-missing: ## Run scraper for missing fighter details only
	.venv/bin/python scripts/filter_missing_fighters.py
	@if [ -f data/processed/fighters_missing.jsonl ] && [ -s data/processed/fighters_missing.jsonl ]; then \
		.venv/bin/scrapy crawl fighter_detail -a input_file="data/processed/fighters_missing.jsonl"; \
	fi

export-active-fighters: ## Export active UFC fighters to JSON for Sherdog matching
	.venv/bin/python -m scripts.export_active_fighters

export-active-fighters-sample: ## Export sample fighters for testing (10 fighters)
	.venv/bin/python -m scripts.export_active_fighters --limit 10

scrape-sherdog-search: ## Search Sherdog for UFC fighters and calculate match confidence
	.venv/bin/scrapy crawl sherdog_search

verify-sherdog-matches: ## Interactive CLI to verify ambiguous Sherdog matches
	.venv/bin/python -m scripts.verify_sherdog_matches

verify-sherdog-matches-auto: ## Non-interactive verification (auto-approve â‰¥70% confidence only)
	.venv/bin/python -m scripts.verify_sherdog_matches --non-interactive

scrape-sherdog-images: ## Download fighter images from Sherdog
	.venv/bin/scrapy crawl sherdog_images

scrape-sherdog-details: ## Scrape detailed fighter stats from Sherdog (DOB, height, reach, etc.)
	.venv/bin/scrapy crawl sherdog_detail

scrape-sherdog-details-sample: ## Scrape Sherdog details for sample fighters (min confidence 70%)
	.venv/bin/scrapy crawl sherdog_detail -a min_confidence=70 -s CLOSESPIDER_ITEMCOUNT=10

update-fighter-images: ## Update database with Sherdog IDs and image paths
	.venv/bin/python -m scripts.update_fighter_images

sherdog-workflow: ## Run complete Sherdog image scraping workflow (interactive)
	@echo "Step 1: Exporting active fighters..."
	@$(MAKE) export-active-fighters
	@echo "\nStep 2: Searching Sherdog for matches..."
	@$(MAKE) scrape-sherdog-search
	@echo "\nStep 3: Verifying matches (interactive)..."
	@$(MAKE) verify-sherdog-matches
	@echo "\nStep 4: Downloading images..."
	@$(MAKE) scrape-sherdog-images
	@echo "\nStep 5: Updating database..."
	@$(MAKE) update-fighter-images
	@echo "\nâœ“ Sherdog workflow complete!"

sherdog-workflow-auto: ## Run complete Sherdog workflow (non-interactive, auto-approve â‰¥70%)
	@echo "Step 1: Exporting active fighters..."
	@$(MAKE) export-active-fighters
	@echo "\nStep 2: Searching Sherdog for matches..."
	@$(MAKE) scrape-sherdog-search
	@echo "\nStep 3: Verifying matches (auto-approve â‰¥70% confidence)..."
	@$(MAKE) verify-sherdog-matches-auto
	@echo "\nStep 4: Downloading images..."
	@$(MAKE) scrape-sherdog-images
	@echo "\nStep 5: Updating database..."
	@$(MAKE) update-fighter-images
	@echo "\nâœ“ Sherdog auto workflow complete!"

sherdog-workflow-sample: ## Run Sherdog workflow with sample data (10 fighters)
	@echo "Step 1: Exporting sample fighters..."
	@$(MAKE) export-active-fighters-sample
	@echo "\nStep 2: Searching Sherdog for matches..."
	@$(MAKE) scrape-sherdog-search
	@echo "\nStep 3: Verifying matches (interactive)..."
	@$(MAKE) verify-sherdog-matches
	@echo "\nStep 4: Downloading images..."
	@$(MAKE) scrape-sherdog-images
	@echo "\nStep 5: Updating database..."
	@$(MAKE) update-fighter-images
	@echo "\nâœ“ Sherdog sample workflow complete!"

scrape-images-wikimedia: ## Scrape fighter images from Wikimedia Commons (legal, ~20% coverage)
	PYTHONPATH=. .venv/bin/python scripts/wikimedia_image_scraper.py --batch-size 50

scrape-images-wikimedia-test: ## Test Wikimedia scraper with 5 fighters
	PYTHONPATH=. .venv/bin/python scripts/wikimedia_image_scraper.py --test

scrape-images-orchestrator: ## Multi-source image scraper (Wikimedia â†’ Sherdog mapping)
	PYTHONPATH=. .venv/bin/python scripts/image_scraper_orchestrator.py --batch-size 50

scrape-images-orchestrator-test: ## Test orchestrator with 10 fighters
	PYTHONPATH=. .venv/bin/python scripts/image_scraper_orchestrator.py --test

scrape-images-orchestrator-all: ## Run orchestrator on ALL missing fighters (no batch limit)
	@echo "âš ï¸  This will process ALL fighters missing images (~292)"
	@echo "   Rate limited to 3 seconds per fighter (~15 minutes total)"
	@read -p "Continue? [y/N] " -n 1 -r; \
	echo; \
	if [[ $$REPLY =~ ^[Yy]$$ ]]; then \
		PYTHONPATH=. .venv/bin/python scripts/image_scraper_orchestrator.py --batch-size 1000; \
	fi

sync-images-to-db: ## Sync images on disk to database (additions + deletions)
	PYTHONPATH=. .venv/bin/python scripts/sync_images_to_db.py

review-recent-images: ## Review recently downloaded images (last 24 hours)
	PYTHONPATH=. .venv/bin/python scripts/review_recent_images.py

remove-bad-images: ## Remove bad images and reset database (edit script first!)
	PYTHONPATH=. .venv/bin/python scripts/remove_bad_images.py

normalize-images: ## Normalize all fighter images to consistent size (300x300 JPEG)
	PYTHONPATH=. .venv/bin/python scripts/normalize_fighter_images.py

normalize-images-dry-run: ## Preview normalization without modifying images
	PYTHONPATH=. .venv/bin/python scripts/normalize_fighter_images.py --dry-run

detect-placeholders: ## Detect Sherdog placeholder images using perceptual hashing
	PYTHONPATH=. .venv/bin/python scripts/detect_placeholder_images.py

detect-placeholders-with-names: ## Detect placeholders and show fighter names
	PYTHONPATH=. .venv/bin/python scripts/detect_placeholder_images.py --with-names

detect-duplicate-photos: ## Find duplicate/similar photos across different fighters
	PYTHONPATH=. .venv/bin/python scripts/detect_duplicate_photos.py

detect-duplicate-photos-exact: ## Find only exact duplicate photos (faster)
	PYTHONPATH=. .venv/bin/python scripts/detect_duplicate_photos.py --exact-only

detect-duplicate-photos-strict: ## Find very similar photos (stricter matching)
	PYTHONPATH=. .venv/bin/python scripts/detect_duplicate_photos.py --similarity 3

review-duplicates: ## Interactive review of duplicate photos with image previews
	PYTHONPATH=. .venv/bin/python scripts/review_duplicates.py

review-duplicates-ascii: ## Review duplicates with ASCII art previews (universal)
	PYTHONPATH=. .venv/bin/python scripts/review_duplicates.py --method ascii

validate-images: ## Validate all fighter images using basic checks
	PYTHONPATH=. .venv/bin/python scripts/validate_fighter_images.py

validate-images-from-file: ## Validate specific fighter IDs from file
	PYTHONPATH=. .venv/bin/python scripts/validate_fighter_images.py --ids-file data/placeholder_fighter_ids.txt

verify-replacement: ## Verify recently replaced placeholder images (last 2 hours)
	@echo "ğŸ” Verifying recently replaced images..."
	@PYTHONPATH=. .venv/bin/python scripts/verify_replacement.py

replace-placeholders: ## Replace Sherdog placeholder images with Bing images (batch of 50)
	PYTHONPATH=. .venv/bin/python scripts/replace_placeholder_images.py --batch-size 50 --yes

replace-placeholders-all: ## Replace ALL placeholder images (may take 1+ hours)
	@echo "âš ï¸  This will replace all 266 placeholder images"
	@echo "   Estimated time: 1-1.5 hours"
	@read -p "Continue? [y/N] " -n 1 -r; \
	echo; \
	if [[ $$REPLY =~ ^[Yy]$$ ]]; then \
		PYTHONPATH=. .venv/bin/python scripts/replace_placeholder_images.py --batch-size 300 --yes; \
	fi

dev-local: ensure-docker ## Start backend + frontend with localhost URLs (no tunnels)
	@echo ""
	@echo "ğŸ”„ Stopping existing processes..."
	@lsof -ti :3000 | xargs kill -9 2>/dev/null || true
	@lsof -ti :8000 | xargs kill -9 2>/dev/null || true
	@sleep 1
	@echo ""
	@echo "âš™ï¸  Configuring localhost environment overrides..."
	@mkdir -p frontend
	@printf '# Auto-generated by `make dev-local` - DO NOT COMMIT\n# Switch back to Cloudflare tunnel URLs with `make dev`\n\nNEXT_PUBLIC_API_BASE_URL=http://localhost:8000\nNEXT_PUBLIC_ASSETS_BASE_URL=http://localhost:8000\n' > frontend/.env.local
	@echo "âœ… Configured frontend/.env.local for localhost access"
	@echo ""
	@echo "ğŸš€ Starting services with localhost configuration..."
	@echo ""
	@echo "Starting backend..."
	@.venv/bin/uvicorn backend.main:app --reload --host $${API_HOST:-0.0.0.0} --port $${API_PORT:-8000} > /tmp/backend.log 2>&1 &
	@sleep 2
	@echo "ğŸ”§ Generating TypeScript types from OpenAPI (background)..."
	@mkdir -p frontend/src/lib/generated
	@( \
	  for i in 1 2 3 4 5 6 7 8 9 10; do \
	    if curl -sf http://localhost:8000/health > /dev/null 2>&1; then \
	      echo "âœ… Backend ready for type generation"; \
	      cd frontend && pnpm generate:types > /tmp/types.log 2>&1 \
	        && echo "âœ… Types generated" \
	        || echo "âš ï¸  Type generation failed (see /tmp/types.log)"; \
	      exit 0; \
	    fi; \
	    sleep 0.5; \
	  done; \
	  echo "âš ï¸  Backend not ready after 5s, skipping type generation"; \
	) &
	@echo "Starting frontend..."
	@cd frontend && pnpm dev > /tmp/frontend.log 2>&1 &
	@sleep 3
	@echo ""
	@echo "âœ… All services started!"
	@echo ""
	@echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
	@echo "ğŸ“ URLs:"
	@echo "   Frontend: http://localhost:3000"
	@echo "   Backend:  http://localhost:8000"
	@echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
	@echo ""
	@echo "ğŸ“ Logs:"
	@echo "   Backend:  tail -f /tmp/backend.log"
	@echo "   Frontend: tail -f /tmp/frontend.log"
	@echo ""
	@echo "Press Ctrl+C to view logs (services will keep running in background)"
	@echo ""
	@trap 'echo ""; echo "Services are still running. To stop them, run: make stop"' INT; \
	tail -f /tmp/backend.log /tmp/frontend.log

stop: ## Stop all running services (backend, frontend, tunnels)
	@echo "ğŸ›‘ Stopping all services..."
	@lsof -ti :3000 | xargs kill -9 2>/dev/null && echo "  âœ“ Frontend stopped" || echo "  - No frontend running"
	@lsof -ti :8000 | xargs kill -9 2>/dev/null && echo "  âœ“ Backend stopped" || echo "  - No backend running"
	@pkill cloudflared 2>/dev/null && echo "  âœ“ Tunnels stopped" || echo "  - No tunnels running"
	@echo ""
	@echo "âœ… All services stopped"
	@echo "ğŸ’¡ Note: Docker services (PostgreSQL, Redis) are still running"
	@echo "   To stop them: make docker-down"

dev-clean: ## Clean frontend caches and restart dev servers (fixes webpack cache issues)
	@echo "ğŸ§¹ Cleaning frontend build caches and restarting..."
	@echo ""
	@$(MAKE) stop
	@echo ""
	@echo "ğŸ—‘ï¸  Removing .next, .turbo, and node_modules/.cache..."
	@rm -rf frontend/.next frontend/.turbo frontend/node_modules/.cache
	@echo "  âœ“ Caches cleaned"
	@echo ""
	@$(MAKE) dev-local

frontend: ## Start only the Next.js frontend (kills existing process on port 3000)
	@echo "Stopping any existing process on port 3000..."
	@lsof -ti :3000 | xargs kill -9 2>/dev/null || true
	@sleep 1
	@echo "Starting frontend..."
	@cd frontend && pnpm dev

types-generate: ## Generate TypeScript types from OpenAPI schema (requires backend running)
	@echo "ğŸ”§ Generating TypeScript types from OpenAPI..."
	@if ! curl -s http://localhost:8000/health > /dev/null 2>&1; then \
		echo "âŒ Backend is not running on http://localhost:8000"; \
		echo "   Please start the backend first with: make api or make api-dev"; \
		exit 1; \
	fi
	@mkdir -p frontend/src/lib/generated
	@cd frontend && pnpm generate:types || npm run generate:types
	@echo "âœ… Types generated at frontend/src/lib/generated/api-schema.ts"

tunnel-frontend: ## Start Cloudflare tunnel for frontend (port 3000)
	cloudflared tunnel --url http://localhost:3000

tunnel-api: ## Start Cloudflare tunnel for API (port 8000)
	cloudflared tunnel --url http://localhost:8000

api-tunnel: tunnel-api ## Alias for tunnel-api

tunnel-stop: ## Stop all Cloudflare tunnels
	@echo "Stopping all Cloudflare tunnels..."
	@pkill cloudflared 2>/dev/null && echo "âœ“ Tunnels stopped" || echo "No tunnels running"

db-upgrade: ## Run database migrations (apply schema changes)
	.venv/bin/python -m alembic upgrade head

db-downgrade: ## Rollback last database migration
	.venv/bin/python -m alembic downgrade -1

db-reset: ## Drop and recreate database (WARNING: destroys all data)
	docker-compose down -v
	docker-compose up -d db
	@echo "Waiting for database to be ready..."
	@sleep 5
	make db-upgrade

load-data: ## Load scraped fighter data (basic list) into database
	.venv/bin/python -m scripts.load_scraped_data $(LOAD_DATA_ARGS)

load-data-sample: ## Load sample fighter data (first 10 fighters)
	.venv/bin/python -m scripts.load_scraped_data --limit 10

load-data-details: ## Load all fighters with full details (from individual JSON files)
	.venv/bin/python -m scripts.load_scraped_data --load-details

load-data-dry-run: ## Validate scraped data without inserting into database
	.venv/bin/python -m scripts.load_scraped_data --dry-run

load-data-details-dry-run: ## Validate detailed fighter data without inserting
	.venv/bin/python -m scripts.load_scraped_data --load-details --dry-run

reload-data: ## Reload fighters list and detail data into database
	@$(MAKE) load-data LOAD_DATA_ARGS="--load-details"

update-records: ## Fast update of fighter records only (~1.5 min for all fighters)
	.venv/bin/python -m scripts.update_fighter_records

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# CHAMPION DATA OPERATIONS
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

champions-scrape: ## Scrape Wikipedia champions and update database
	.venv/bin/python scripts/champions_wiki.py

champions-refresh: ## Full refresh: scrape champions, update DB, regenerate types
	@echo "ğŸ† Refreshing champion data..."
	@$(MAKE) champions-scrape
	@echo "ğŸ”„ Regenerating TypeScript types..."
	@$(MAKE) types-generate
	@echo "âœ… Champion data refreshed!"

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# EVENT DATA OPERATIONS
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

scraper-events: ## Scrape UFC events list (upcoming + completed)
	.venv/bin/scrapy crawl events_list

scraper-events-details: ## Scrape detailed fight cards for all events
	@echo "Starting event detail scraper for all events..."
	@echo "This will take ~5-10 minutes (using cache where available)"
	.venv/bin/scrapy crawl event_detail -a input_file="data/processed/events_list.jsonl"

scraper-events-details-sample: ## Scrape fight cards for first 5 events (testing)
	.venv/bin/scrapy crawl event_detail -a input_file="data/processed/events_list.jsonl" -a limit=5

load-events: ## Load scraped event data into database
	PYTHONPATH=. .venv/bin/python scripts/load_events.py

load-events-sample: ## Load sample event data (first 10 events)
	PYTHONPATH=. .venv/bin/python scripts/load_events.py --limit 10

load-events-dry-run: ## Validate event data without inserting into database
	PYTHONPATH=. .venv/bin/python scripts/load_events.py --dry-run

load-events-details: ## Load event details (fight cards) into database
	PYTHONPATH=. .venv/bin/python scripts/load_event_details.py

load-events-details-sample: ## Load sample event details (first 10 events)
	PYTHONPATH=. .venv/bin/python scripts/load_event_details.py --limit 10

load-events-details-dry-run: ## Validate event details without inserting
	PYTHONPATH=. .venv/bin/python scripts/load_event_details.py --dry-run

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# DEPLOYMENT (cPanel via SSH)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

deploy-config: ## Create deployment configuration from template
	@if [ ! -f .deployment/config.env ]; then \
		cp .deployment/config.env.example .deployment/config.env; \
		echo "âœ“ Created .deployment/config.env"; \
		echo "âš ï¸  Please edit .deployment/config.env with your SSH credentials"; \
	else \
		echo "âš ï¸  .deployment/config.env already exists"; \
	fi

deploy-check: ## Test SSH connection to cPanel server
	@bash scripts/test_ssh.sh

deploy-build: ## Build frontend for production (static export)
	@echo "Building Next.js for production..."
	@cd frontend && BUILD_MODE=static npm run build:static
	@echo "âœ“ Build complete: frontend/out/"

deploy-test: deploy-build ## Build and test deployment locally
	@echo "Starting local server to preview build..."
	@cd frontend/out && python3 -m http.server 8080

deploy: ## Deploy to cPanel subdomain (builds and uploads via SSH)
	@if [ ! -f .deployment/config.env ]; then \
		echo "âŒ Error: .deployment/config.env not found"; \
		echo "Run: make deploy-config"; \
		exit 1; \
	fi
	@bash scripts/deploy.sh

deploy-ftp: ## Deploy to cPanel subdomain via FTP (recommended if SSH fails)
	@if [ ! -f .deployment/config.env ]; then \
		echo "âŒ Error: .deployment/config.env not found"; \
		echo "Run: make deploy-config"; \
		exit 1; \
	fi
	@bash scripts/deploy_ftp.sh

deploy-ssh: ## Deploy to cPanel via SSH (faster and more reliable)
	@bash scripts/deploy_ssh.sh

deploy-ssh-test: ## Test SSH connection to cPanel
	@echo "Testing SSH connection..."
	@SSHPASS='EuroBender2024!' sshpass -e ssh -p 21098 -o StrictHostKeyChecking=no wolfdgpl@162.254.39.96 'echo "âœ“ SSH connection successful!"; pwd; ls -la'
